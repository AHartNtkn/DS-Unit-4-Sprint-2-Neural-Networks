{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVfaLrjLvxvQ"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 1*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wxtoY12mwmih"
   },
   "source": [
    "## Define the Following:\n",
    "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
    "\n",
    "### Input Layer: The layer to a network where external values are fed.\n",
    "### Hidden Layer: A layer between the input and output layers where hidden stuff happens\n",
    "### Output Layer: The final layer of a network which results are read from.\n",
    "### Neuron: A node in a neural network with associated activation and connecting weights\n",
    "### Weight: A connection between two neurons dictating the degree to which a signal is scaled\n",
    "### Activation Function: A function used to calculate the output value of a neuron, often used for the purposes of scaling neuron \n",
    "### Node Map: A diagramatic presentation of a neural network depicing neurons as small circles and weights as connecting lines\n",
    "### Perceptron: A kind of neural network with a simple input fed to an outpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXuy9WcWzxa4"
   },
   "source": [
    "## Inputs -> Outputs\n",
    "\n",
    "### Explain the flow of information through a neural network from inputs to outputs. Be sure to include: inputs, weights, bias, and activation functions. How does it all flow from beginning to end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlSwIJMC0A8F"
   },
   "source": [
    "Training data starts at the input layer. the input layer then activates using connecting weights to other layers. The hidden layers receive these signals, then relay them, modify them using appropriate weights, applying their activation functions, then sending them to the next layer. At each layer, a bias term is applied, perterbing the whole signal in some direction. The final layer is the output layer, from which the results from the network can be read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6sWR43PTwhSk"
   },
   "source": [
    "## Write your own perceptron code that can correctly classify (99.0% accuracy) a NAND gate. \n",
    "\n",
    "| x1 | x2 | y |\n",
    "|----|----|---|\n",
    "| 0  | 0  | 1 |\n",
    "| 1  | 0  | 1 |\n",
    "| 0  | 1  | 1 |\n",
    "| 1  | 1  | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = { 'x1': [0,1,0,1],\n",
    "         'x2': [0,0,1,1],\n",
    "         'y':  [1,1,1,0]\n",
    "       }\n",
    "\n",
    "df = pd.DataFrame.from_dict(data).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sgh7VFGwnXGH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Your Code Here #####\n",
    "per = Perceptron(niter = 10000)\n",
    "\n",
    "X = (np.array([data['x1'], data['x2']]).T)\n",
    "y = (np.array([data['y']]).T)\n",
    "\n",
    "per.fit(X, y)\n",
    "\n",
    "np.round(per.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-12.55254133],\n",
       "       [-12.55254133]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diabetes = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although neural networks can handle non-normalized data, scaling or normalizing your data will improve your neural network's learning speed. Try to apply the sklearn `MinMaxScaler` or `Normalizer` to your diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
    "\n",
    "feats = list(diabetes)[:-1]\n",
    "\n",
    "X = np.array(diabetes[feats])\n",
    "y = np.array(diabetes[['Outcome']])\n",
    "\n",
    "mms = MinMaxScaler()\n",
    "X = mms.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "per = Perceptron(niter = 100000)\n",
    "\n",
    "per.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.round(per.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(pred, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-W0tiX1F1hh2"
   },
   "outputs": [],
   "source": [
    "##### Update this Class #####\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __relu(x):\n",
    "        if x >= 0:\n",
    "            return x\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def __relu_array(x):\n",
    "        return np.vectorize(Perceptron.__relu)(x)\n",
    "\n",
    "    def __relu_derivative(x):\n",
    "        if x >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def __relu_derivative_array(x):\n",
    "        return np.vectorize(Perceptron.__relu_derivative)(x)\n",
    "    \n",
    "    def __sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(x):\n",
    "        sx = Perceptron.__sigmoid(x)\n",
    "        return sx * (1-sx)\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        self.niter = niter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # X : n x m\n",
    "        # y : n x 1\n",
    "        \n",
    "        # Randomly Initialize Weights\n",
    "        # weights : m x 1\n",
    "        self.weights = 2 * np.random.random((len(X.T), 1)) - 1\n",
    "        \n",
    "        # Randomly initialized bias\n",
    "        # bias : n x 1\n",
    "        self.bias = 2 * np.random.random() - 1\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            #print(\"\\nweights\\n\", self.weights)\n",
    "            \n",
    "            # Weighted sum of inputs / weights\n",
    "            # weighted_sum : (n x m) ; (m x 1) = n x 1\n",
    "            weighted_sum = np.dot(X, self.weights) + self.bias\n",
    "            #print(\"\\nweighted_sum\\n\", weighted_sum)\n",
    "            \n",
    "            # activated : n x 1\n",
    "            activated = Perceptron.__sigmoid(weighted_sum)\n",
    "            #print(\"\\nactivated\\n\", activated)\n",
    "\n",
    "            # Cac error\n",
    "            # error : n x 1\n",
    "            error = y - activated\n",
    "            #print(\"\\nerror\\n\", error)\n",
    "\n",
    "            # Update the Weights\n",
    "            # adjusted : n x 1\n",
    "            adjusted = error * Perceptron.__sigmoid_derivative(activated)\n",
    "            #print(\"\\nadjusted\\n\", adjusted)\n",
    "            \n",
    "            # weights : (m x n) ; (n x 1) = m x 1\n",
    "            self.weights += np.dot(X.T, adjusted)\n",
    "            self.bias += np.mean(error)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        weighted_sum = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        return Perceptron.__sigmoid(weighted_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QR4oAW1xdyu"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Research \"backpropagation\" to learn how weights get updated in neural networks (tomorrow's lecture). \n",
    "- Implement a multi-layer perceptron. (for non-linearly separable classes)\n",
    "- Try and implement your own backpropagation algorithm.\n",
    "- What are the pros and cons of the different activation functions? How should you decide between them for the different layers of a neural network?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_431_Intro_to_NN_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
